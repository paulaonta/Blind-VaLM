[2024-08-04 11:46:51,698] torch.distributed.run: [WARNING] 
[2024-08-04 11:46:51,698] torch.distributed.run: [WARNING] *****************************************
[2024-08-04 11:46:51,698] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-08-04 11:46:51,698] torch.distributed.run: [WARNING] *****************************************
/ikerlariak/pontalvilla001/VaLM/fairseq/fairseq/distributed/utils.py:620: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)
/ikerlariak/pontalvilla001/VaLM/fairseq/fairseq/distributed/utils.py:620: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)
/ikerlariak/pontalvilla001/VaLM/fairseq/fairseq/distributed/utils.py:620: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)
/ikerlariak/pontalvilla001/VaLM/fairseq/fairseq/distributed/utils.py:620: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)
wandb: Currently logged in as: paula-ontalvilla (paulaixa). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /ikerlariak/pontalvilla001/VaLM/wandb/run-20240804_114726-ku790ebt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run checkpoint_gpt_medium_lrx4_40600upd
wandb: â­ï¸ View project at https://wandb.ai/paulaixa/VaLM-baseline
wandb: ğŸš€ View run at https://wandb.ai/paulaixa/VaLM-baseline/runs/ku790ebt
/ikerlariak/pontalvilla001/VaLM/VaLM_ve_image/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/ikerlariak/pontalvilla001/VaLM/VaLM_ve_image/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/ikerlariak/pontalvilla001/VaLM/VaLM_ve_image/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/ikerlariak/pontalvilla001/VaLM/VaLM_ve_image/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
wandb: - 0.016 MB of 0.016 MB uploadedwandb: \ 0.016 MB of 0.016 MB uploadedwandb: | 0.016 MB of 0.016 MB uploadedwandb: / 0.016 MB of 0.016 MB uploadedwandb: - 0.025 MB of 0.430 MB uploadedwandb: \ 0.430 MB of 0.430 MB uploadedwandb: 
wandb: Run history:
wandb:              train/bsz â–â–â–â–â–â–â–
wandb:             train/clip â–ˆâ–â–â–â–â–â–
wandb:          train/gb_free â–â–â–â–â–â–â–
wandb:            train/gnorm â–…â–â–„â–…â–†â–ˆâ–ˆ
wandb:             train/loss â–ˆâ–ƒâ–‚â–‚â–â–â–
wandb:       train/loss_scale â–ƒâ–ƒâ–â–â–ˆâ–â–
wandb:               train/lr â–ˆâ–…â–ƒâ–‚â–â–â–
wandb:              train/ppl â–ˆâ–ƒâ–‚â–‚â–â–â–
wandb:       train/train_wall â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:              train/ups â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–
wandb:             train/wall â–â–‚â–„â–…â–‡â–ˆâ–ˆ
wandb:              train/wpb â–ƒâ–â–‚â–‡â–ƒâ–ƒâ–ˆ
wandb:              train/wps â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–
wandb:        train_inner/bsz â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:       train_inner/clip â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    train_inner/gb_free â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      train_inner/gnorm â–ˆâ–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:       train_inner/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: train_inner/loss_scale â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–â–ƒâ–ƒâ–ˆâ–
wandb:         train_inner/lr â–ƒâ–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        train_inner/ppl â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: train_inner/train_wall â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–…â–…â–„â–‡â–‡â–…â–ˆ
wandb:        train_inner/ups â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–†â–„â–„â–…â–‚â–ƒâ–„â–
wandb:       train_inner/wall â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        train_inner/wpb â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:        train_inner/wps â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ƒâ–„â–„â–‚â–ƒâ–„â–
wandb: 
wandb: Run summary:
wandb:              train/bsz 128.0
wandb:             train/clip 0.0
wandb:          train/gb_free 14.9
wandb:            train/gnorm 0.582
wandb:             train/loss 4.345
wandb:       train/loss_scale 8.0
wandb:               train/lr 0.00031
wandb:              train/ppl 20.33
wandb:       train/train_wall 109.0
wandb:              train/ups 0.64
wandb:             train/wall 137694.0
wandb:              train/wpb 65536.0
wandb:              train/wps 42172.1
wandb:        train_inner/bsz 128.0
wandb:       train_inner/clip 0.0
wandb:    train_inner/gb_free 14.9
wandb:      train_inner/gnorm 0.577
wandb:       train_inner/loss 4.346
wandb: train_inner/loss_scale 8.0
wandb:         train_inner/lr 0.00031
wandb:        train_inner/ppl 20.34
wandb: train_inner/train_wall 82.0
wandb:        train_inner/ups 1.2
wandb:       train_inner/wall 137603.0
wandb:        train_inner/wpb 65536.0
wandb:        train_inner/wps 78652.0
wandb: 
wandb: ğŸš€ View run checkpoint_gpt_medium_lrx4_40600upd at: https://wandb.ai/paulaixa/VaLM-baseline/runs/ku790ebt
wandb: â­ï¸ View project at: https://wandb.ai/paulaixa/VaLM-baseline
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240804_114726-ku790ebt/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
