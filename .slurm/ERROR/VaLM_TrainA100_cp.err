[2023-12-18 09:51:29,723] torch.distributed.run: [WARNING] 
[2023-12-18 09:51:29,723] torch.distributed.run: [WARNING] *****************************************
[2023-12-18 09:51:29,723] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-12-18 09:51:29,723] torch.distributed.run: [WARNING] *****************************************
/ikerlariak/pontalvilla001/VaLM/fairseq/fairseq/distributed/utils.py:620: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)
/ikerlariak/pontalvilla001/VaLM/fairseq/fairseq/distributed/utils.py:620: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)
/ikerlariak/pontalvilla001/VaLM/fairseq/fairseq/distributed/utils.py:620: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)
/ikerlariak/pontalvilla001/VaLM/fairseq/fairseq/distributed/utils.py:620: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)
wandb: Currently logged in as: paula-ontalvilla (paulaixa). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /ikerlariak/pontalvilla001/VaLM/wandb/run-20231218_095208-zlobtr9g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run checkpoint_valmA100
wandb: ⭐️ View project at https://wandb.ai/paulaixa/VaLM-baseline
wandb: 🚀 View run at https://wandb.ai/paulaixa/VaLM-baseline/runs/zlobtr9g
/ikerlariak/pontalvilla001/VaLM/VaLM_ve_image/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/ikerlariak/pontalvilla001/VaLM/VaLM_ve_image/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
wandb: Network error (ReadTimeout), entering retry loop.
wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)
/ikerlariak/pontalvilla001/VaLM/VaLM_ve_image/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/ikerlariak/pontalvilla001/VaLM/VaLM_ve_image/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
wandb: 
wandb: Run history:
wandb:              train/bsz ▁▁▁██
wandb:             train/clip █▁▁▁▁
wandb:          train/gb_free ▁▁▁▁▁
wandb:            train/gnorm █▁▁▁▁
wandb:             train/loss █▂▂▁▁
wandb:       train/loss_scale ▁▁▁▁▁
wandb:               train/lr █▄▂▁▁
wandb:              train/ppl █▂▁▁▁
wandb:       train/train_wall ████▁
wandb:              train/ups ▁▁██▁
wandb:             train/wall ▁▃▅▇█
wandb:              train/wpb ▄▁▄█▅
wandb:              train/wps ▁▃█▆▄
wandb:        train_inner/bsz █████████████████████████████████▁██████
wandb:       train_inner/clip █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_inner/gb_free ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_inner/gnorm █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_inner/loss █▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_inner/loss_scale ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_inner/lr ▁▂▃▅▆█▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃
wandb:        train_inner/ppl █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_inner/train_wall ▆█▇▇▃▅▆▅▄▃▆▂█▆▅▁▄▅▂▂▂▃▄▂▂▂▂▃▁▂▁▃▃▃▅▄▄▄▆▅
wandb:        train_inner/ups ▁▁▁▁█▁▁▁▁█▁█▁▁▁█▁▁████████████████▁▁▁▁▁▁
wandb:       train_inner/wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:        train_inner/wpb █████████████████████████████████▁██████
wandb:        train_inner/wps ▂▁▂▂▆▄▃▄▅▆▃▇▁▃▄█▅▃▇▇▇▆▅▇▇▇▇▆█▇█▆▆▆▄▅▅▅▃▄
wandb: 
wandb: Run summary:
wandb:              train/bsz 512.0
wandb:             train/clip 0.0
wandb:          train/gb_free 10.5
wandb:            train/gnorm 0.452
wandb:             train/loss 4.842
wandb:       train/loss_scale 4.0
wandb:               train/lr 0.00183
wandb:              train/ppl 28.68
wandb:       train/train_wall 35036.0
wandb:              train/ups 0.08
wandb:             train/wall 353729.0
wandb:              train/wpb 262123.0
wandb:              train/wps 22231.3
wandb:        train_inner/bsz 512.0
wandb:       train_inner/clip 0.0
wandb:    train_inner/gb_free 10.5
wandb:      train_inner/gnorm 0.457
wandb:       train_inner/loss 4.836
wandb: train_inner/loss_scale 4.0
wandb:         train_inner/lr 0.00183
wandb:        train_inner/ppl 28.56
wandb: train_inner/train_wall 1185.0
wandb:        train_inner/ups 0.08
wandb:       train_inner/wall 353684.0
wandb:        train_inner/wpb 262144.0
wandb:        train_inner/wps 22114.8
wandb: 
wandb: 🚀 View run checkpoint_valmA100 at: https://wandb.ai/paulaixa/VaLM-baseline/runs/zlobtr9g
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231218_095208-zlobtr9g/logs
