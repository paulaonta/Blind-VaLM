Hasi da
2024-04-13 23:45:05 | INFO | fairseq.distributed.utils | setting CUDA device=3 on rank 3
2024-04-13 23:45:05 | INFO | fairseq.distributed.utils | setting CUDA device=1 on rank 1
2024-04-13 23:45:05 | INFO | fairseq.distributed.utils | setting CUDA device=2 on rank 2
2024-04-13 23:45:05 | INFO | fairseq.distributed.utils | distributed init (rank 0): env://
2024-04-13 23:45:05 | INFO | fairseq.distributed.utils | initialized host durunda as rank 0
2024-04-13 23:45:06 | INFO | fairseq.distributed.utils | distributed init (rank 3): env://
2024-04-13 23:45:06 | INFO | fairseq.distributed.utils | initialized host durunda as rank 3
2024-04-13 23:45:06 | INFO | fairseq.distributed.utils | distributed init (rank 1): env://
2024-04-13 23:45:06 | INFO | fairseq.distributed.utils | initialized host durunda as rank 1
2024-04-13 23:45:06 | INFO | fairseq.distributed.utils | distributed init (rank 2): env://
2024-04-13 23:45:06 | INFO | fairseq.distributed.utils | initialized host durunda as rank 2
NCCL version 2.18.1+cuda12.1
{'_name': 'language_modeling', 'data': './data/finetune_data', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}
2024-04-13 23:45:15 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'VaLM-baseline', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 65536, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': True, 'max_tokens_valid': 65536, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 46600, 'stop_time_hours': 0.0, 'clip_norm': 2.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune', 'restore_file': './CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd/checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_gpt', 'activation_fn': 'gelu', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 768, 'decoder_output_dim': 768, 'decoder_input_dim': 768, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 12, 'decoder_attention_heads': 12, 'decoder_normalize_before': True, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False, 'use_knn_datastore': False, 'load_knn_datastore': False, 'dstore_fp16': False, 'use_gpu_to_search': False, 'move_dstore_to_mem': False, 'dstore_size': 10000000, 'k': 8, 'probe': 32, 'dstore_filename': 'data/datastore', 'use_joint_attention': False, 'joint_layer_index': 2}, 'task': {'_name': 'language_modeling', 'data': './data/finetune_data', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
{'_name': 'language_modeling', 'data': './data/finetune_data', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}
{'_name': 'language_modeling', 'data': './data/finetune_data', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}
{'_name': 'language_modeling', 'data': './data/finetune_data', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}
2024-04-13 23:45:15 | INFO | fairseq.tasks.language_modeling | dictionary: 49412 types
2024-04-13 23:45:16 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(49412, 768, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=49412, bias=False)
  )
)
2024-04-13 23:45:16 | INFO | fairseq_cli.train | task: LanguageModelingTask
2024-04-13 23:45:16 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2024-04-13 23:45:16 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2024-04-13 23:45:16 | INFO | fairseq_cli.train | num. shared model params: 123,004,416 (num. trained: 123,004,416)
2024-04-13 23:45:16 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2024-04-13 23:45:17 | INFO | fairseq.data.data_utils | loaded 42,694,945 examples from: ./data/finetune_data/valid
2024-04-13 23:45:17 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2024-04-13 23:45:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2024-04-13 23:45:26 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.325 GB ; name = NVIDIA A100-SXM4-80GB                   
2024-04-13 23:45:26 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 79.325 GB ; name = NVIDIA A100-SXM4-80GB                   
2024-04-13 23:45:26 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 79.325 GB ; name = NVIDIA A100-SXM4-80GB                   
2024-04-13 23:45:26 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 79.325 GB ; name = NVIDIA A100-SXM4-80GB                   
2024-04-13 23:45:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2024-04-13 23:45:26 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2024-04-13 23:45:26 | INFO | fairseq_cli.train | max tokens per device = 65536 and max sentences per device = None
2024-04-13 23:45:26 | INFO | fairseq.trainer | Preparing to load checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd/checkpoint_last.pt
2024-04-13 23:45:27 | INFO | fairseq.trainer | Loaded checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd/checkpoint_last.pt (epoch 7 @ 40600 updates)
2024-04-13 23:45:27 | INFO | fairseq.trainer | loading train data for epoch 7
2024-04-13 23:45:28 | INFO | fairseq.data.data_utils | loaded 42,694,945 examples from: ./data/finetune_data/train
2024-04-13 23:45:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5435
2024-04-13 23:45:31 | INFO | fairseq.trainer | begin training epoch 7
2024-04-13 23:45:31 | INFO | fairseq_cli.train | Start iterating over samples
2024-04-13 23:46:32 | INFO | train_inner | epoch 007:    160 / 5435 loss=5.239, ppl=37.75, wps=404485, ups=1.54, wpb=262144, bsz=512, num_updates=40700, lr=0.000626993, gnorm=0.379, clip=0, loss_scale=4, train_wall=61, gb_free=12.8, wall=0
2024-04-13 23:47:32 | INFO | train_inner | epoch 007:    260 / 5435 loss=5.222, ppl=37.32, wps=436266, ups=1.66, wpb=262144, bsz=512, num_updates=40800, lr=0.000626224, gnorm=0.33, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:48:32 | INFO | train_inner | epoch 007:    360 / 5435 loss=5.218, ppl=37.22, wps=436333, ups=1.66, wpb=262144, bsz=512, num_updates=40900, lr=0.000625458, gnorm=0.333, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:49:32 | INFO | train_inner | epoch 007:    460 / 5435 loss=5.213, ppl=37.1, wps=436377, ups=1.66, wpb=262144, bsz=512, num_updates=41000, lr=0.000624695, gnorm=0.309, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:49:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 41000 updates
2024-04-13 23:49:32 | INFO | fairseq.trainer | Saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_41000.pt
2024-04-13 23:49:47 | INFO | fairseq.trainer | Finished saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_41000.pt
2024-04-13 23:50:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_41000.pt (epoch 7 @ 41000 updates, score None) (writing took 43.400529865175486 seconds)
2024-04-13 23:51:16 | INFO | train_inner | epoch 007:    560 / 5435 loss=5.211, ppl=37.04, wps=253602, ups=0.97, wpb=262144, bsz=512, num_updates=41100, lr=0.000623935, gnorm=0.324, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:52:16 | INFO | train_inner | epoch 007:    660 / 5435 loss=5.21, ppl=37.01, wps=436400, ups=1.66, wpb=262144, bsz=512, num_updates=41200, lr=0.000623177, gnorm=0.322, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:53:16 | INFO | train_inner | epoch 007:    760 / 5435 loss=5.207, ppl=36.94, wps=436556, ups=1.67, wpb=262144, bsz=512, num_updates=41300, lr=0.000622422, gnorm=0.313, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:54:16 | INFO | train_inner | epoch 007:    860 / 5435 loss=5.207, ppl=36.93, wps=436306, ups=1.66, wpb=262139, bsz=512, num_updates=41400, lr=0.00062167, gnorm=0.306, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:55:16 | INFO | train_inner | epoch 007:    960 / 5435 loss=5.204, ppl=36.85, wps=436610, ups=1.67, wpb=262144, bsz=512, num_updates=41500, lr=0.00062092, gnorm=0.317, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:56:16 | INFO | train_inner | epoch 007:   1060 / 5435 loss=5.205, ppl=36.89, wps=436773, ups=1.67, wpb=262144, bsz=512, num_updates=41600, lr=0.000620174, gnorm=0.326, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:57:16 | INFO | train_inner | epoch 007:   1160 / 5435 loss=5.203, ppl=36.83, wps=436292, ups=1.66, wpb=262144, bsz=512, num_updates=41700, lr=0.00061943, gnorm=0.329, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:58:16 | INFO | train_inner | epoch 007:   1260 / 5435 loss=5.201, ppl=36.79, wps=436373, ups=1.66, wpb=262144, bsz=512, num_updates=41800, lr=0.000618688, gnorm=0.318, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-13 23:59:16 | INFO | train_inner | epoch 007:   1360 / 5435 loss=5.202, ppl=36.81, wps=435753, ups=1.67, wpb=261499, bsz=510.7, num_updates=41900, lr=0.000617949, gnorm=0.31, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:00:16 | INFO | train_inner | epoch 007:   1460 / 5435 loss=5.202, ppl=36.8, wps=436673, ups=1.67, wpb=262144, bsz=512, num_updates=42000, lr=0.000617213, gnorm=0.315, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 42000 updates
2024-04-14 00:00:16 | INFO | fairseq.trainer | Saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_42000.pt
2024-04-14 00:00:31 | INFO | fairseq.trainer | Finished saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_42000.pt
2024-04-14 00:00:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_42000.pt (epoch 7 @ 42000 updates, score None) (writing took 43.250151799060404 seconds)
2024-04-14 00:01:59 | INFO | train_inner | epoch 007:   1560 / 5435 loss=5.199, ppl=36.74, wps=253841, ups=0.97, wpb=262144, bsz=512, num_updates=42100, lr=0.00061648, gnorm=0.32, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:02:59 | INFO | train_inner | epoch 007:   1660 / 5435 loss=5.2, ppl=36.75, wps=436726, ups=1.67, wpb=262144, bsz=512, num_updates=42200, lr=0.000615749, gnorm=0.324, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:04:00 | INFO | train_inner | epoch 007:   1760 / 5435 loss=5.201, ppl=36.78, wps=436467, ups=1.66, wpb=262144, bsz=512, num_updates=42300, lr=0.000615021, gnorm=0.323, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:05:00 | INFO | train_inner | epoch 007:   1860 / 5435 loss=5.198, ppl=36.71, wps=436455, ups=1.66, wpb=262144, bsz=512, num_updates=42400, lr=0.000614295, gnorm=0.31, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:06:00 | INFO | train_inner | epoch 007:   1960 / 5435 loss=5.198, ppl=36.7, wps=436481, ups=1.67, wpb=262144, bsz=512, num_updates=42500, lr=0.000613572, gnorm=0.307, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:07:00 | INFO | train_inner | epoch 007:   2060 / 5435 loss=5.195, ppl=36.63, wps=436432, ups=1.66, wpb=262144, bsz=512, num_updates=42600, lr=0.000612851, gnorm=0.316, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:08:00 | INFO | train_inner | epoch 007:   2160 / 5435 loss=5.194, ppl=36.6, wps=436268, ups=1.66, wpb=262144, bsz=512, num_updates=42700, lr=0.000612133, gnorm=0.312, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:09:00 | INFO | train_inner | epoch 007:   2260 / 5435 loss=5.199, ppl=36.73, wps=436225, ups=1.66, wpb=262144, bsz=512, num_updates=42800, lr=0.000611418, gnorm=0.316, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:10:00 | INFO | train_inner | epoch 007:   2360 / 5435 loss=5.193, ppl=36.58, wps=436362, ups=1.66, wpb=262144, bsz=512, num_updates=42900, lr=0.000610705, gnorm=0.31, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:11:00 | INFO | train_inner | epoch 007:   2460 / 5435 loss=5.194, ppl=36.6, wps=436262, ups=1.66, wpb=262144, bsz=512, num_updates=43000, lr=0.000609994, gnorm=0.317, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:11:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 43000 updates
2024-04-14 00:11:00 | INFO | fairseq.trainer | Saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_43000.pt
2024-04-14 00:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_43000.pt
2024-04-14 00:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_43000.pt (epoch 7 @ 43000 updates, score None) (writing took 43.54798652883619 seconds)
2024-04-14 00:12:44 | INFO | train_inner | epoch 007:   2560 / 5435 loss=5.194, ppl=36.6, wps=253178, ups=0.97, wpb=262144, bsz=512, num_updates=43100, lr=0.000609286, gnorm=0.305, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:13:44 | INFO | train_inner | epoch 007:   2660 / 5435 loss=5.195, ppl=36.62, wps=436468, ups=1.66, wpb=262144, bsz=512, num_updates=43200, lr=0.000608581, gnorm=0.318, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:14:44 | INFO | train_inner | epoch 007:   2760 / 5435 loss=5.19, ppl=36.51, wps=436437, ups=1.66, wpb=262144, bsz=512, num_updates=43300, lr=0.000607877, gnorm=0.321, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:15:44 | INFO | train_inner | epoch 007:   2860 / 5435 loss=5.191, ppl=36.53, wps=436177, ups=1.66, wpb=262144, bsz=512, num_updates=43400, lr=0.000607177, gnorm=0.314, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:16:44 | INFO | train_inner | epoch 007:   2960 / 5435 loss=5.189, ppl=36.49, wps=436249, ups=1.66, wpb=262144, bsz=512, num_updates=43500, lr=0.000606478, gnorm=0.308, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:17:44 | INFO | train_inner | epoch 007:   3060 / 5435 loss=5.189, ppl=36.49, wps=436403, ups=1.66, wpb=262144, bsz=512, num_updates=43600, lr=0.000605783, gnorm=0.318, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:18:44 | INFO | train_inner | epoch 007:   3160 / 5435 loss=5.192, ppl=36.55, wps=436813, ups=1.67, wpb=262144, bsz=512, num_updates=43700, lr=0.000605089, gnorm=0.333, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:19:44 | INFO | train_inner | epoch 007:   3260 / 5435 loss=5.19, ppl=36.51, wps=436809, ups=1.67, wpb=262144, bsz=512, num_updates=43800, lr=0.000604398, gnorm=0.309, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:20:44 | INFO | train_inner | epoch 007:   3360 / 5435 loss=5.191, ppl=36.53, wps=436678, ups=1.67, wpb=262144, bsz=512, num_updates=43900, lr=0.000603709, gnorm=0.316, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:21:44 | INFO | train_inner | epoch 007:   3460 / 5435 loss=5.189, ppl=36.49, wps=436526, ups=1.67, wpb=262144, bsz=512, num_updates=44000, lr=0.000603023, gnorm=0.304, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 44000 updates
2024-04-14 00:21:44 | INFO | fairseq.trainer | Saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_44000.pt
2024-04-14 00:21:59 | INFO | fairseq.trainer | Finished saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_44000.pt
2024-04-14 00:22:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_44000.pt (epoch 7 @ 44000 updates, score None) (writing took 55.116789150983095 seconds)
2024-04-14 00:23:39 | INFO | train_inner | epoch 007:   3560 / 5435 loss=5.189, ppl=36.47, wps=227776, ups=0.87, wpb=262144, bsz=512, num_updates=44100, lr=0.000602339, gnorm=0.318, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:24:39 | INFO | train_inner | epoch 007:   3660 / 5435 loss=5.189, ppl=36.47, wps=436295, ups=1.66, wpb=262144, bsz=512, num_updates=44200, lr=0.000601657, gnorm=0.322, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:25:39 | INFO | train_inner | epoch 007:   3760 / 5435 loss=5.189, ppl=36.48, wps=436571, ups=1.67, wpb=262144, bsz=512, num_updates=44300, lr=0.000600977, gnorm=0.326, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:26:39 | INFO | train_inner | epoch 007:   3860 / 5435 loss=5.186, ppl=36.41, wps=436678, ups=1.67, wpb=262144, bsz=512, num_updates=44400, lr=0.0006003, gnorm=0.322, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:27:39 | INFO | train_inner | epoch 007:   3960 / 5435 loss=5.187, ppl=36.43, wps=436275, ups=1.66, wpb=262144, bsz=512, num_updates=44500, lr=0.000599625, gnorm=0.32, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:28:40 | INFO | train_inner | epoch 007:   4060 / 5435 loss=5.185, ppl=36.38, wps=436405, ups=1.66, wpb=262144, bsz=512, num_updates=44600, lr=0.000598953, gnorm=0.312, clip=0, loss_scale=4, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:29:40 | INFO | train_inner | epoch 007:   4160 / 5435 loss=5.187, ppl=36.43, wps=436580, ups=1.67, wpb=262144, bsz=512, num_updates=44700, lr=0.000598282, gnorm=0.31, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:30:40 | INFO | train_inner | epoch 007:   4260 / 5435 loss=5.186, ppl=36.41, wps=436212, ups=1.66, wpb=262144, bsz=512, num_updates=44800, lr=0.000597614, gnorm=0.307, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:31:40 | INFO | train_inner | epoch 007:   4360 / 5435 loss=5.184, ppl=36.35, wps=436351, ups=1.66, wpb=262144, bsz=512, num_updates=44900, lr=0.000596948, gnorm=0.313, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:32:40 | INFO | train_inner | epoch 007:   4460 / 5435 loss=5.185, ppl=36.37, wps=436344, ups=1.66, wpb=262144, bsz=512, num_updates=45000, lr=0.000596285, gnorm=0.305, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:32:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 45000 updates
2024-04-14 00:32:40 | INFO | fairseq.trainer | Saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_45000.pt
2024-04-14 00:32:55 | INFO | fairseq.trainer | Finished saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_45000.pt
2024-04-14 00:33:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_7_45000.pt (epoch 7 @ 45000 updates, score None) (writing took 43.43697162531316 seconds)
2024-04-14 00:34:23 | INFO | train_inner | epoch 007:   4560 / 5435 loss=5.184, ppl=36.36, wps=253358, ups=0.97, wpb=262144, bsz=512, num_updates=45100, lr=0.000595623, gnorm=0.329, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:35:23 | INFO | train_inner | epoch 007:   4660 / 5435 loss=5.184, ppl=36.35, wps=436166, ups=1.66, wpb=262144, bsz=512, num_updates=45200, lr=0.000594964, gnorm=0.31, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:36:23 | INFO | train_inner | epoch 007:   4760 / 5435 loss=5.181, ppl=36.28, wps=436225, ups=1.66, wpb=262144, bsz=512, num_updates=45300, lr=0.000594307, gnorm=0.315, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:37:24 | INFO | train_inner | epoch 007:   4860 / 5435 loss=5.183, ppl=36.34, wps=436056, ups=1.66, wpb=262144, bsz=512, num_updates=45400, lr=0.000593652, gnorm=0.316, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:38:24 | INFO | train_inner | epoch 007:   4960 / 5435 loss=5.183, ppl=36.32, wps=435593, ups=1.66, wpb=261898, bsz=511.5, num_updates=45500, lr=0.000592999, gnorm=0.306, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:39:24 | INFO | train_inner | epoch 007:   5060 / 5435 loss=5.181, ppl=36.28, wps=436204, ups=1.66, wpb=262144, bsz=512, num_updates=45600, lr=0.000592349, gnorm=0.304, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:40:24 | INFO | train_inner | epoch 007:   5160 / 5435 loss=5.181, ppl=36.28, wps=435989, ups=1.66, wpb=262144, bsz=512, num_updates=45700, lr=0.0005917, gnorm=0.324, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:41:24 | INFO | train_inner | epoch 007:   5260 / 5435 loss=5.182, ppl=36.3, wps=436039, ups=1.66, wpb=262144, bsz=512, num_updates=45800, lr=0.000591054, gnorm=0.305, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:42:24 | INFO | train_inner | epoch 007:   5360 / 5435 loss=5.183, ppl=36.32, wps=436216, ups=1.66, wpb=262144, bsz=512, num_updates=45900, lr=0.00059041, gnorm=0.309, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:43:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 45975 updates
2024-04-14 00:43:09 | INFO | fairseq.trainer | Saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint7.pt
2024-04-14 00:43:24 | INFO | fairseq.trainer | Finished saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint7.pt
2024-04-14 00:43:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint7.pt (epoch 7 @ 45975 updates, score None) (writing took 43.516190306283534 seconds)
2024-04-14 00:43:53 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2024-04-14 00:43:53 | INFO | train | epoch 007 | loss 5.19 | ppl 36.51 | wps 402089 | ups 1.53 | wpb 262128 | bsz 512 | num_updates 45975 | lr 0.000589928 | gnorm 0.317 | clip 0 | loss_scale 8 | train_wall 3255 | gb_free 12.8 | wall 0
2024-04-14 00:43:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 5435
2024-04-14 00:43:53 | INFO | fairseq.trainer | begin training epoch 8
2024-04-14 00:43:53 | INFO | fairseq_cli.train | Start iterating over samples
2024-04-14 00:44:08 | INFO | train_inner | epoch 008:     25 / 5435 loss=5.172, ppl=36.06, wps=252876, ups=0.96, wpb=262144, bsz=512, num_updates=46000, lr=0.000589768, gnorm=0.313, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:44:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 46000 updates
2024-04-14 00:44:08 | INFO | fairseq.trainer | Saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_8_46000.pt
2024-04-14 00:44:23 | INFO | fairseq.trainer | Finished saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_8_46000.pt
2024-04-14 00:44:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_8_46000.pt (epoch 8 @ 46000 updates, score None) (writing took 43.67026777006686 seconds)
2024-04-14 00:45:51 | INFO | train_inner | epoch 008:    125 / 5435 loss=5.154, ppl=35.61, wps=252977, ups=0.97, wpb=262144, bsz=512, num_updates=46100, lr=0.000589128, gnorm=0.31, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:46:51 | INFO | train_inner | epoch 008:    225 / 5435 loss=5.155, ppl=35.63, wps=436664, ups=1.67, wpb=262144, bsz=512, num_updates=46200, lr=0.00058849, gnorm=0.337, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:47:52 | INFO | train_inner | epoch 008:    325 / 5435 loss=5.158, ppl=35.71, wps=436221, ups=1.66, wpb=262144, bsz=512, num_updates=46300, lr=0.000587854, gnorm=0.328, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:48:52 | INFO | train_inner | epoch 008:    425 / 5435 loss=5.154, ppl=35.6, wps=436407, ups=1.66, wpb=262144, bsz=512, num_updates=46400, lr=0.00058722, gnorm=0.321, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:49:52 | INFO | train_inner | epoch 008:    525 / 5435 loss=5.158, ppl=35.71, wps=436123, ups=1.66, wpb=262144, bsz=512, num_updates=46500, lr=0.000586588, gnorm=0.322, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:50:52 | INFO | train_inner | epoch 008:    625 / 5435 loss=5.159, ppl=35.73, wps=436204, ups=1.66, wpb=262144, bsz=512, num_updates=46600, lr=0.000585959, gnorm=0.306, clip=0, loss_scale=8, train_wall=60, gb_free=12.8, wall=0
2024-04-14 00:50:52 | INFO | fairseq_cli.train | Stopping training due to num_updates: 46600 >= max_update: 46600
2024-04-14 00:50:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 46600 updates
2024-04-14 00:50:52 | INFO | fairseq.trainer | Saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_last.pt
2024-04-14 00:51:07 | INFO | fairseq.trainer | Finished saving checkpoint to ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_last.pt
2024-04-14 00:51:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./CHECKPOINTS/checkpoint_gpt_blind_lrx4_40600upd_more_finetune/checkpoint_last.pt (epoch 8 @ 46600 updates, score None) (writing took 15.324517868459225 seconds)
2024-04-14 00:51:07 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2024-04-14 00:51:07 | INFO | train | epoch 008 | loss 5.156 | ppl 35.66 | wps 377104 | ups 1.44 | wpb 262144 | bsz 512 | num_updates 46600 | lr 0.000585959 | gnorm 0.32 | clip 0 | loss_scale 8 | train_wall 374 | gb_free 12.8 | wall 0
2024-04-14 00:51:07 | INFO | fairseq_cli.train | done training in 3938.7 seconds
Amaitu da
